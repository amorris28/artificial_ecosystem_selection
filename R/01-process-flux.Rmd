---
title: "Process Flux Data"
author: "Andrew Morris"
date: "`r Sys.Date()`"
output: html_document
---

This script processes the standard curve data from the GC on each sample date. It then converts area to methane concentration using the standard curves for every sample. Finally, it computes fluxes (change in concentration over time) as a first-order exponential decay function.

```{r initialize_packages, message=FALSE}
library(tidyverse)
library(broom)
library(lubridate)
library(knitr)
```

```{r initialize_variables}
source('functions.R')
```


```{r import_data}
sc <- read.csv(paste0(raw_dir, 'standard_curve.csv'))
sc_dates <- read.csv(paste0(raw_dir, 'sc_dates.csv'))
time_data <- data.table::fread(paste0(raw_dir, 'time_data.csv'))
conc_data <- data.table::fread(paste0(raw_dir, 'conc_data.csv'))
selected <- read.csv(paste0(raw_dir, 'selected.csv'))
```

This dataset includes `r length(unique(substr(conc_data$jar, 1, 1)))` treatments across `r length(unique(conc_data$flux_date))` passages (generations). This includes a total of n = `r nrow(conc_data)` samples. However, passage 6 will not be included in the analyzed dataset, because that passage was sampled much later due to covid and used different potting mix so has too many confounding factors. The samples are broken down across passages and treatments as in the following table:


```{r echo=FALSE}
conc_data %>% 
  mutate(treat = substr(conc_data$jar, 1, 1),
         rep = substr(conc_data$jar, 2, 3)) %>% 
  group_by(flux_date, treat) %>% 
  summarize(n = n_distinct(rep), .groups='drop') %>% 
  kable()
```


## Standard Curves

This step first imports the values for the standard curves then calculates the slope of Methane on Area for each sample date.

```{r standard_curves}
sc_model <- function(df) {lm(injection_ppm ~ area, data = df)}

# fit a separate model for each date
standard_curves <- 
  sc %>% 
  filter(molecule == 'ch4') %>% 
  group_by(flux_date, sc_date) %>% 
  nest() %>% 
  mutate(model = map(data, sc_model),
         glance = map(model, glance),
         tidy = map(model, tidy)) 

sc_r2 <- 
  standard_curves %>% 
  unnest(glance) %>% 
  select(flux_date, sc_date, r.squared)

sc_equation <- 
  standard_curves %>% 
  unnest(tidy) %>%
  select(flux_date, sc_date, term, estimate) %>% 
  spread(term, estimate) %>% 
  rename(intercept = `(Intercept)`, slope = area)

sc_ch4 <- left_join(sc_r2, sc_equation, by = c("flux_date", "sc_date"))

# These are the final slopes for each Methane~Area standard curve 
# model for each date
sc_ch4 <- 
  sc_ch4 %>% 
  left_join(sc_dates, by = "sc_date") %>% 
  select(flux_date:sc_date, t, r.squared:slope)
```

## Compute Fluxes

This step turns the time point data to time intervals in units of days.

```{r remove_p6}
conc_data <- 
  conc_data %>% 
  filter(flux_date != '2020-07-27')
time_data <- 
  time_data %>% 
  filter(flux_date != '2020-07-27')

```

```{r compute_time_intervals}

time_data[, c('t0', 't1', 't2', 't3', 't4', 't5')] <- 
  lapply(time_data[, c('t0', 't1', 't2', 't3', 't4', 't5')],
         ymd_hms)

as.days <- function(start, end) {as.numeric(interval(start, end))/60/60/24}

time_data[, c('t1', 't2', 't3', 't4', 't5')] <- 
  lapply(time_data[, c('t1', 't2', 't3', 't4', 't5')],
         function(x) as.days(time_data$t1, x))
time_data <- 
 time_data %>% 
  select(-t0) %>% 
  gather(t, days, t1:t5) %>% 
  mutate(flux_date = factor(flux_date))

```

This step takes the raw gas chromatograph area data and calculates methane concentration for each time point in units of ppm.

```{r compute_ch4_conc}
conc_data <- 
  conc_data %>% 
  filter(molecule == 'ch4') %>% 
  mutate(flux_date = factor(flux_date)) %>% 
  gather(t, area, t1:t5) %>% 
  left_join(sc_ch4, by = c('flux_date', 't')) %>% 
  mutate(ppm = area * slope + intercept) %>% 
  select(flux_date, jar, t, ppm) %>% 
  spread(t, ppm)
```

```{r plot_fractions}

conc_data %>% 
  select(flux_date, jar, t1, t4) %>% 
  mutate(frac_cons = t4/t1) %>% 
  mutate(treat = substring(jar,1,1)) %>% 
  ggplot(aes(x = flux_date, y = frac_cons, color = treat)) +
  geom_jitter(width = 0.2) +
  geom_hline(yintercept = 0.90) +
  geom_hline(yintercept = 0.05)

```

Finally, this step takes the time intervals and ch4 concentrations and fits a first-order exponential decay function to compute methane flux as `k` which is the rate-constant for the first-order exponential decay function.

```{r fit_flux}
ln_ch0_chn <- function(t0, tn) {
  log(t0/tn)
}

conc_data[, c('t1', 't2', 't3', 't4', 't5')] <- 
  lapply(conc_data[, c('t1', 't2', 't3', 't4', 't5')],
         function(x) ln_ch0_chn(conc_data$t1, x))

flux_data <-
  conc_data %>% 
  gather(t, ppm, t1:t5) %>% 
  left_join(time_data, by = c('flux_date', 'jar', 't'))

flux_data <- 
  flux_data %>% 
  mutate(flux_date = factor(ymd(flux_date)), 
         jar = factor(jar),
         t = factor(t)) %>% 
  mutate(rep = factor(substr(jar, 2, 3)),
         treat = factor(substr(jar, 1, 1)))

lin_model <- function(dt) {
  lm(ppm ~ days, data = dt)
}

nested <- 
  flux_data %>% 
  group_by(flux_date, jar, treat, rep) %>% 
  nest() %>% 
  mutate(model = map(data, lin_model))

fluxes <-
  nested %>% 
  mutate(tidy = map(model, tidy)) %>% 
  unnest(tidy) %>% 
  filter(term == 'days') %>% 
  group_by(flux_date) %>% 
  arrange(estimate, .by_group = TRUE) %>% 
  select(-data, -model, -term) %>% 
  ungroup() %>% 
  mutate(passage = as.numeric(flux_date)) %>% 
  mutate(ch4 = estimate,
         rank_ch4 = rank(ch4),
         log_ch4 = log10(ch4 + abs(min(ch4)) + 0.001)) %>% 
  select(-std.error, -statistic, -p.value, -estimate)
```

```{r plot_fluxes, fig.height=20}
flux_data %>%
 ggplot(aes(days, ppm)) +
 facet_grid(flux_date + treat ~ rep, scales = "free") +
 geom_point() +
 stat_smooth(method = 'lm', formula = y ~ x)
```

```{r identify_selected}

# Identify the selected jars in the fluxes dataframe
selected$selected <- TRUE
fluxes <- 
  fluxes %>% 
  left_join(selected, by = c('passage', 'jar')) %>% 
  mutate(selected = !is.na(selected)) %>% 
  select(-jar) %>% 
  select(flux_date, passage, treat, rep, ch4:selected)
```

```{r print_data}
kable(fluxes)
```

```{r export_data}
write_tsv(fluxes, paste0(der_dir, 'fluxes.tsv'))
```


The final dataset includes `r length(unique(fluxes$treat))` treatments across `r length(unique(fluxes$passage))` passages (generations). This includes a total of n = `r nrow(fluxes)` samples. The samples are broken down across passages and treatments as in the following table:


```{r echo=FALSE}
fluxes %>% 
  group_by(flux_date, treat) %>% 
  summarize(n = n_distinct(rep), .groups='drop') %>% 
  kable()
```
